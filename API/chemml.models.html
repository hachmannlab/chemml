<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Models module &mdash; ChemML  documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Optimization module" href="chemml.optimization.html" />
    <link rel="prev" title="Preprocessing module" href="chemml.preprocessing.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            ChemML
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ChemML Library</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chemml_library.html">Introduction to ChemML Library</a></li>
<li class="toctree-l1"><a class="reference internal" href="../input_task/index.html">Input</a></li>
<li class="toctree-l1"><a class="reference internal" href="../represent_task/index.html">Represent</a></li>
<li class="toctree-l1"><a class="reference internal" href="../prepare_task/index.html">Prepare</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_task/index.html">Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="../optimize_task/index.html">Optimize</a></li>
<li class="toctree-l1"><a class="reference internal" href="../explain_task/index.html">Explain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../visualize_task/index.html">Visualize</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ChemML Wrapper</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../chemml_wrapper.html">Introduction to ChemML Wrapper</a></li>
<li class="toctree-l1"><a class="reference internal" href="../wrapper/index.html">ChemML Wrapper Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Published Models</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../published/index.html">Published</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ChemML API</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="index.html">ChemML API</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="API.html">Library API documentation</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="chemml.chem.html">Chem module</a></li>
<li class="toctree-l3"><a class="reference internal" href="chemml.chem.magpie_python.html">Magpie_Python module</a></li>
<li class="toctree-l3"><a class="reference internal" href="chemml.initialization.html">Initialization module</a></li>
<li class="toctree-l3"><a class="reference internal" href="chemml.datasets.html">Datasets module</a></li>
<li class="toctree-l3"><a class="reference internal" href="chemml.preprocessing.html">Preprocessing module</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Models module</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#chemml.models.MLP"><code class="docutils literal notranslate"><span class="pre">MLP</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#chemml.models.NeuralGraphHidden"><code class="docutils literal notranslate"><span class="pre">NeuralGraphHidden</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#chemml.models.NeuralGraphOutput"><code class="docutils literal notranslate"><span class="pre">NeuralGraphOutput</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#chemml.models.TransferLearning"><code class="docutils literal notranslate"><span class="pre">TransferLearning</span></code></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="chemml.optimization.html">Optimization module</a></li>
<li class="toctree-l3"><a class="reference internal" href="chemml.visualization.html">Visualization module</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="API.html#wrapper-api-documentation">Wrapper API documentation</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">ChemML</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">ChemML API</a></li>
          <li class="breadcrumb-item"><a href="API.html">Library API documentation</a></li>
      <li class="breadcrumb-item active">Models module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/API/chemml.models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="models-module">
<h1>Models module<a class="headerlink" href="#models-module" title="Permalink to this heading">ÔÉÅ</a></h1>
<span class="target" id="module-chemml.models"></span><p>The chemml.models.keras module includes (please click on links adjacent to function names for more information):</p>
<dl class="py class">
<dt class="sig sig-object py" id="chemml.models.MLP">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">chemml.models.</span></span><span class="sig-name descname"><span class="pre">MLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">engine</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nfeatures</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nneurons</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nepochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean_squared_error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">is_regression</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nclasses</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_config_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'sgd'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">random_seed</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">112</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">params</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Class associated with Multi-Layer Perceptron (Neural Network)</p>
<section id="parameters">
<h2>Parameters<a class="headerlink" href="#parameters" title="Permalink to this heading">ÔÉÅ</a></h2>
<dl class="simple">
<dt>engine: str</dt><dd><p>Determines the underlying ML library used to build the deep neural network
can be either ‚Äòtensorflow‚Äô or ‚Äòpytorch‚Äô</p>
</dd>
<dt>nfeatures: int</dt><dd><p>number of input features</p>
</dd>
<dt>nneurons: list, default = None</dt><dd><p>The number of nodes in each hidden layer, required if layer_config_file is not provided</p>
</dd>
<dt>activations: list, default = None</dt><dd><p>The activation type for each hidden layer (len of activations should be the same as len of nneurons)
required if layer_config_file is not provided
Refer <a class="reference external" href="https://keras.io/activations/">https://keras.io/activations/</a> for list of valid activations for tensorflow
Refer <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity">https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity</a> or 
<a class="reference external" href="https://pytorch.org/docs/stable/nn.html#non-linear-activations-other">https://pytorch.org/docs/stable/nn.html#non-linear-activations-other</a> for list of valid activations 
for pytorch 
e.g. [‚ÄòReLU‚Äô,‚ÄôReLU‚Äô]</p>
</dd>
<dt>nepochs: int, optional, default: 100</dt><dd><p>Number of training epochs.</p>
</dd>
<dt>batch_size: int, optional, default: 100</dt><dd><p>Number of training samples in mini-batch</p>
</dd>
<dt>alpha: float, default: 0.001 (defaults for pytorch: 0, keras: 0.01, sklearn: 0.0001)</dt><dd><p>L2 regularization parameter. 
If engine is pytorch, this will override the weight_decay parameter in the SGD optimizer</p>
</dd>
<dt>loss: str, optional, default: ‚Äòmean_squared_error‚Äô</dt><dd><p>Type of loss used to train the neural network.
Refer <a class="reference external" href="https://keras.io/losses/">https://keras.io/losses/</a> for list of valid losses for tensorflow
Refer <a class="reference external" href="https://pytorch.org/docs/stable/nn.html#loss-functions">https://pytorch.org/docs/stable/nn.html#loss-functions</a> for valid losses for pytorch</p>
</dd>
<dt>regression: bool, optional, default: True</dt><dd><p>Decides whether we are training for regression or classification task</p>
</dd>
<dt>nclasses: int, optional, default: None</dt><dd><p>Number of classes labels needs to be specified if regression is False</p>
</dd>
<dt>layer_config_file: str, optional, default: None</dt><dd><p>Path to the file that specifies layer configuration
Refer MLP test to see a sample file
Note: this variable SHOULD be consolidated with the layers variable to reduce redundancy</p>
</dd>
<dt>opt_config: list or str, optional, default: ‚Äòsgd‚Äô</dt><dd><p>optimizer configuration. 
If str, should either be ‚Äòsgd‚Äô or ‚Äòadam‚Äô
If list, should provide exact configurations and parameters corresponding to the respective engines, for e.g., 
[‚ÄúAdam‚Äù,{‚Äúlearning_rate‚Äù:0.01}] or 
[‚ÄúSGD‚Äù,{‚Äúlr‚Äù:0.01, ‚Äúmomentum‚Äù:0.9, ‚Äúlr_decay‚Äù:0.0, nesterov=False)]</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.get_model">
<span class="sig-name descname"><span class="pre">get_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">include_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.get_model" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Returns the entire tensorflow or pytorch model in its current state (fitted or compiled)</p>
<section id="id1">
<h3>Parameters<a class="headerlink" href="#id1" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="simple">
<dt>include_output: bool</dt><dd><p>if True it will return the entire model, if False it will return 
the model without the output layer</p>
</dd>
<dt>n_layers: int, optional (default=None)</dt><dd><p>remove the last ‚Äòn‚Äô hidden layers from the model in addition to the output layer. 
Note that this number should not include the output layer.</p>
</dd>
</dl>
</section>
<section id="returns">
<h3>Returns<a class="headerlink" href="#returns" title="Permalink to this heading">ÔÉÅ</a></h3>
<p>self.model: tensorflow.python.keras type object or torch.nn.Module type object</p>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path_to_model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.load" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Loads the chemml.models.MLP object along with the underlying 
tensorflow.python.keras object</p>
<section id="id2">
<h3>Parameters<a class="headerlink" href="#id2" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="simple">
<dt>path_to_model: str</dt><dd><p>path to the chemml.models.MLP csv file</p>
</dd>
</dl>
</section>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.save" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Saves the chemml.models.MLP object along with the underlying 
tensorflow.python.keras object</p>
<section id="id3">
<h3>Parameters<a class="headerlink" href="#id3" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="simple">
<dt>path: str</dt><dd><p>the path to the directory where the models should be saved</p>
</dd>
<dt>filename: str</dt><dd><p>the name of the model file without the file type</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">chemml.models.</span></span><span class="sig-name descname"><span class="pre">NeuralGraphHidden</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><dl>
<dt>Hidden Convolutional layer in a Neural Graph (as in Duvenaud et. al.,</dt><dd><p>2015). This layer takes a graph as an input. The graph is represented as by
three tensors.
- The atoms tensor represents the features of the nodes.
- The bonds tensor represents the features of the edges.
- The edges tensor represents the connectivity (which atoms are connected to</p>
<blockquote>
<div><p>which)</p>
</div></blockquote>
<p>It returns the convolved features tensor, which is very similar to the atoms
tensor. Instead of each node being represented by a num_atom_features-sized
vector, each node now is represented by a convolved feature vector of size
conv_width.
# Example</p>
<blockquote>
<div><p>Define the input:
<a href="#id4"><span class="problematic" id="id5">``</span></a><a href="#id6"><span class="problematic" id="id7">`</span></a>python</p>
<blockquote>
<div><p>atoms0 = Input(name=‚Äôatom_inputs‚Äô, shape=(max_atoms, num_atom_features))
bonds = Input(name=‚Äôbond_inputs‚Äô, shape=(max_atoms, max_degree, num_bond_features))
edges = Input(name=‚Äôedge_inputs‚Äô, shape=(max_atoms, max_degree), dtype=‚Äôint32‚Äô)</p>
</div></blockquote>
<p><a href="#id8"><span class="problematic" id="id9">``</span></a>`
The <cite>NeuralGraphHidden</cite> can be initialised in three ways:
1. Using an integer <cite>conv_width</cite> and possible kwags (<cite>Dense</cite> layer is used)</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">atoms1</span> <span class="pre">=</span> <span class="pre">NeuralGraphHidden(conv_width,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False)([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><dl class="simple">
<dt>Using an initialised <cite>Dense</cite> layer</dt><dd><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">atoms1</span> <span class="pre">=</span> <span class="pre">NeuralGraphHidden(Dense(conv_width,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False))([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Using a function that returns an initialised <cite>Dense</cite> layer</dt><dd><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">atoms1</span> <span class="pre">=</span> <span class="pre">NeuralGraphHidden(lambda:</span> <span class="pre">Dense(conv_width,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False))([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</li>
</ol>
<p>Use <cite>NeuralGraphOutput</cite> to convert atom layer to fingerprint</p>
</div></blockquote>
<dl>
<dt># Arguments</dt><dd><dl class="simple">
<dt>inner_layer_arg: Either:</dt><dd><ol class="arabic simple">
<li><dl class="simple">
<dt>an int defining the <cite>conv_width</cite>, with optional kwargs for the</dt><dd><p>inner Dense layer</p>
</dd>
</dl>
</li>
<li><p>An initialised but not build (<cite>Dense</cite>) keras layer (like a wrapper)</p></li>
<li><p>A function that returns an initialised keras layer.</p></li>
</ol>
</dd>
</dl>
<p>kwargs: For initialisation 1. you can pass <cite>Dense</cite> layer kwargs</p>
</dd>
<dt># Input shape</dt><dd><p>List of Atom and edge tensors of shape:
<a href="#id10"><span class="problematic" id="id11">`</span></a>[(samples, max_atoms, atom_features), (samples, max_atoms, max_degrees,</p>
<blockquote>
<div><p>bond_features), (samples, max_atoms, max_degrees)]`</p>
</div></blockquote>
<p>where degrees referes to number of neighbours</p>
</dd>
<dt># Output shape</dt><dd><p>New atom featuers of shape
<cite>(samples, max_atoms, conv_width)</cite></p>
</dd>
<dt># References</dt><dd><ul class="simple">
<li><p>[Convolutional Networks on Graphs for Learning Molecular Fingerprints](<a class="reference external" href="https://arxiv.org/abs/1509.09292">https://arxiv.org/abs/1509.09292</a>)</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden.build" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden.call" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This is where the layer‚Äôs logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id12"><span class="problematic" id="id13">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden.compute_output_shape">
<span class="sig-name descname"><span class="pre">compute_output_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden.compute_output_shape" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>If the layer has not been built, this method will call <cite>build</cite> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Shape tuple (tuple of integers)</dt><dd><p>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>An input shape tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden.from_config">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden.from_config" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Creates a layer from its config.</p>
<p>This method is the reverse of <cite>get_config</cite>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <cite>set_weights</cite>).</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>config: A Python dictionary, typically the</dt><dd><p>output of get_config.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A layer instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden.get_config" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">chemml.models.</span></span><span class="sig-name descname"><span class="pre">NeuralGraphOutput</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Output Convolutional layer in a Neural Graph (as in Duvenaud et. al.,
2015). This layer takes a graph as an input. The graph is represented as by
three tensors.</p>
<ul class="simple">
<li><p>The atoms tensor represents the features of the nodes.</p></li>
<li><p>The bonds tensor represents the features of the edges.</p></li>
<li><dl class="simple">
<dt>The edges tensor represents the connectivity (which atoms are connected to</dt><dd><p>which)</p>
</dd>
</dl>
</li>
</ul>
<p>It returns the fingerprint vector for each sample for the given layer.</p>
<p>According to the original paper, the fingerprint outputs of each hidden layer
need to be summed in the end to come up with the final fingerprint.</p>
<dl>
<dt># Example</dt><dd><p>Define the input:
<a href="#id14"><span class="problematic" id="id15">``</span></a><a href="#id16"><span class="problematic" id="id17">`</span></a>python</p>
<blockquote>
<div><p>atoms0 = Input(name=‚Äôatom_inputs‚Äô, shape=(max_atoms, num_atom_features))
bonds = Input(name=‚Äôbond_inputs‚Äô, shape=(max_atoms, max_degree, num_bond_features))
edges = Input(name=‚Äôedge_inputs‚Äô, shape=(max_atoms, max_degree), dtype=‚Äôint32‚Äô)</p>
</div></blockquote>
<p><a href="#id18"><span class="problematic" id="id19">``</span></a><a href="#id20"><span class="problematic" id="id21">`</span></a></p>
<p>The <cite>NeuralGraphOutput</cite> can be initialised in three ways:
1. Using an integer <cite>fp_length</cite> and possible kwags (<cite>Dense</cite> layer is used)</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">fp_out</span> <span class="pre">=</span> <span class="pre">NeuralGraphOutput(fp_length,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False)([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><dl class="simple">
<dt>Using an initialised <cite>Dense</cite> layer</dt><dd><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">fp_out</span> <span class="pre">=</span> <span class="pre">NeuralGraphOutput(Dense(fp_length,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False))([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Using a function that returns an initialised <cite>Dense</cite> layer</dt><dd><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">fp_out</span> <span class="pre">=</span> <span class="pre">NeuralGraphOutput(lambda:</span> <span class="pre">Dense(fp_length,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False))([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</li>
</ol>
<p>Predict for regression:
<code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">main_prediction</span> <span class="pre">=</span> <span class="pre">Dense(1,</span> <span class="pre">activation='linear',</span> <span class="pre">name='main_prediction')(fp_out)</span>
<span class="pre">`</span></code></p>
</dd>
<dt># Arguments</dt><dd><dl class="simple">
<dt>inner_layer_arg: Either:</dt><dd><ol class="arabic simple">
<li><dl class="simple">
<dt>an int defining the <cite>fp_length</cite>, with optional kwargs for the</dt><dd><p>inner Dense layer</p>
</dd>
</dl>
</li>
<li><p>An initialised but not build (<cite>Dense</cite>) keras layer (like a wrapper)</p></li>
<li><p>A function that returns an initialised keras layer.</p></li>
</ol>
</dd>
</dl>
<p>kwargs: For initialisation 1. you can pass <cite>Dense</cite> layer kwargs</p>
</dd>
<dt># Input shape</dt><dd><p>List of Atom and edge tensors of shape:
<a href="#id22"><span class="problematic" id="id23">`</span></a>[(samples, max_atoms, atom_features), (samples, max_atoms, max_degrees,</p>
<blockquote>
<div><p>bond_features), (samples, max_atoms, max_degrees)]`</p>
</div></blockquote>
<p>where degrees referes to number of neighbours</p>
</dd>
<dt># Output shape</dt><dd><p>Fingerprints matrix
<cite>(samples, fp_length)</cite></p>
</dd>
<dt># References</dt><dd><ul class="simple">
<li><p>[Convolutional Networks on Graphs for Learning Molecular Fingerprints](<a class="reference external" href="https://arxiv.org/abs/1509.09292">https://arxiv.org/abs/1509.09292</a>)</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput.build" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput.call" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>This is where the layer‚Äôs logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><p>inputs: Input tensor, or list/tuple of input tensors.
<a href="#id24"><span class="problematic" id="id25">**</span></a>kwargs: Additional keyword arguments. Currently unused.</p>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput.compute_output_shape">
<span class="sig-name descname"><span class="pre">compute_output_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput.compute_output_shape" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>If the layer has not been built, this method will call <cite>build</cite> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>input_shape: Shape tuple (tuple of integers)</dt><dd><p>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>An input shape tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput.from_config">
<em class="property"><span class="pre">classmethod</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">from_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput.from_config" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Creates a layer from its config.</p>
<p>This method is the reverse of <cite>get_config</cite>,
capable of instantiating the same layer from the config
dictionary. It does not handle layer connectivity
(handled by Network), nor weights (handled by <cite>set_weights</cite>).</p>
<dl class="simple">
<dt>Arguments:</dt><dd><dl class="simple">
<dt>config: A Python dictionary, typically the</dt><dd><p>output of get_config.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A layer instance.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput.get_config" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="chemml.models.TransferLearning">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-prename descclassname"><span class="pre">chemml.models.</span></span><span class="sig-name descname"><span class="pre">TransferLearning</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nfeatures</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_layers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.TransferLearning" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Class used to facilitate transfer learning from a parent (or head) model to a child model.
Freezes the layers (weights and biases) of the pre-trained base 
model and removes the output layers and appends the child model to the base model.</p>
<section id="id26">
<h2>Parameters<a class="headerlink" href="#id26" title="Permalink to this heading">ÔÉÅ</a></h2>
<dl class="simple">
<dt>base_model: chemml.models.MLP object, tensorflow.python.keras object, or a torch.nn.Module object</dt><dd><p>pre-trained base model</p>
</dd>
<dt>n_features: int or None (default=None)</dt><dd><p>no. of input features provided for training the base model. 
If base_model is a tensorflow.python.keras object or torch.nn.Modules object, then n_features must be int, else it 
can be None.</p>
</dd>
<dt>n_layers: int, optional (default=None)</dt><dd><p>remove the last ‚Äòn‚Äô hidden layers from the base model. 
Note that this number should not include the output layer.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.TransferLearning.transfer">
<span class="sig-name descname"><span class="pre">transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">child_model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.TransferLearning.transfer" title="Permalink to this definition">ÔÉÅ</a></dt>
<dd><p>Adds the base model‚Äôs frozen layers (without its input and output layers) 
to the child model and fits the new model to the training data.</p>
<section id="id27">
<h3>Parameters<a class="headerlink" href="#id27" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="simple">
<dt>X, y: numpy.ndarray</dt><dd><p>X is an array of features and y is an array of the target values
X should have the same input features (columns) as the base model.</p>
</dd>
<dt>child_model: chemml.models.mlp.MLP object</dt><dd><p>chemml model created with all the parameters and options required for 
the final transfer learned model</p>
</dd>
</dl>
</section>
<section id="id28">
<h3>Returns<a class="headerlink" href="#id28" title="Permalink to this heading">ÔÉÅ</a></h3>
<dl class="simple">
<dt>child_model: chemml.models.mlp.MLP object</dt><dd><p>The trained transfer-learned child model.</p>
</dd>
</dl>
</section>
</dd></dl>

</section>
</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="chemml.preprocessing.html" class="btn btn-neutral float-left" title="Preprocessing module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="chemml.optimization.html" class="btn btn-neutral float-right" title="Optimization module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2014-2022 Johannes Hachmann, Mojtaba Haghighatlari, Aditya Sonpal, Gaurav Vishwakarma and Aatish Pradhan.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>