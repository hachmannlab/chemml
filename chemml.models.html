<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Models module &mdash; ChemML  documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Optimization module" href="chemml.optimization.html" />
    <link rel="prev" title="Preprocessing module" href="chemml.preprocessing.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> ChemML
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ChemML Library</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/Molecule.html">Molecule</a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/feature_representation.html">Feature Representation Methods in ChemML</a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/ml_model.html">Build a Neural Network using ChemML</a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/ga_hyper_opt.html">Hyperparameter Optimization using <code class="docutils literal notranslate"><span class="pre">chemml.optimization.GeneticAlgorithm</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/ga_feature_selection.html">Feature Selection using <code class="docutils literal notranslate"><span class="pre">chemml.optimization.GeneticAlgorithm</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/active_model_based.html">Active Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/Convo_nets.html">Neural Fingerprints</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ChemML Wrapper</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="CMLWTutorial.html">ChemML Wrapper Tutorial</a></li>
<li class="toctree-l1"><a class="reference internal" href="CMLWInputFile.html">Input File Manual</a></li>
<li class="toctree-l1"><a class="reference internal" href="CMLWInputFileGenerator.html">Input File GUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/read_excel.html">Generate Morgan fingerprints from SMILES codes</a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/pyscript_usage.html">Pyscript Usage</a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/simple_ml_model.html">Build a simple ML model</a></li>
<li class="toctree-l1"><a class="reference internal" href="ipython_notebooks/GA_tutorial.html">Genetic Algorithm GUI tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">ChemML API</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="API.html">Library API documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="API.html#wrapper-api-documentation">Wrapper API documentation</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">ChemML</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="API.html">Library API documentation</a> &raquo;</li>
      <li>Models module</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/chemml.models.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<section id="models-module">
<h1>Models module<a class="headerlink" href="#models-module" title="Permalink to this headline"></a></h1>
<span class="target" id="module-chemml.models"></span><dl class="simple">
<dt>The ‘chemml.models’ module includes (please click on links adjacent to function names for more information):</dt><dd><ul class="simple">
<li><p>OrganicLorentzLorenz: <code class="xref py py-func docutils literal notranslate"><span class="pre">OrganicLorentzLorenz()</span></code></p></li>
<li><p>MLP: <a class="reference internal" href="#chemml.models.MLP" title="chemml.models.keras.mlp.MLP"><code class="xref py py-func docutils literal notranslate"><span class="pre">MLP()</span></code></a></p></li>
</ul>
</dd>
</dl>
<dl class="py class">
<dt class="sig sig-object py" id="chemml.models.MLP">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">chemml.models.</span></span><span class="sig-name descname"><span class="pre">MLP</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">nhidden</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nneurons</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">activations</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">learning_rate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">lr_decay</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nepochs</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">loss</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'mean_squared_error'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">regression</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">nclasses</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">layer_config_file</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opt_config</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP" title="Permalink to this definition"></a></dt>
<dd><p>Class associated with Multi-Layer Perceptron (Neural Network)</p>
<dl class="simple">
<dt>nhidden<span class="classifier">int, optional, default: 1</span></dt><dd><p>The number of hidden layers in the neural network (excluding input and output)</p>
</dd>
<dt>nneurons: list, optional, default: [100] * nhidden</dt><dd><p>The number of nodes in each hidden layer. Must be of same length as nhidden</p>
</dd>
<dt>activations: list, optional, default: [‘sigmoid’] * nhidden</dt><dd><p>The activation type for each hidden layer. Must be of same length as nhidden.
Refer <a class="reference external" href="https://keras.io/activations/">https://keras.io/activations/</a> for list of valid activations</p>
</dd>
<dt>nepochs: int, optional, default: 100</dt><dd><p>Number of training epochs.</p>
</dd>
<dt>batch_size: int, optional, default: 100</dt><dd><p>Number of training samples in mini-batch</p>
</dd>
<dt>loss: str, optional, default: ‘mean_squared_error’</dt><dd><p>Type of loss used to train the neural network.
Refer <a class="reference external" href="https://keras.io/losses/">https://keras.io/losses/</a> for list of valid losses</p>
</dd>
<dt>regression: bool, optional, default: True</dt><dd><p>Decides whether we are training for regression or classification task</p>
</dd>
<dt>nclasses: int, optional, default: None</dt><dd><p>Number of classes labels needs to be specified if regression is False</p>
</dd>
<dt>layer_config_file: str, optional, default: None</dt><dd><p>Path to the file that specifies layer configuration
Refer MLP test to see a sample file</p>
</dd>
<dt>opt_config: list, optional, default: None</dt><dd><p>optimizer configuration (for e.g., [“Adam”,{“learning_rate”:0.01}] or 
[“SGD”,{“lr”:0.01, “momentum”:0.9, “lr_decay”:0.0, nesterov=False)
Refer MLP test to see a sample file</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.fit" title="Permalink to this definition"></a></dt>
<dd><p>Train the MLP for training data X and targets y</p>
<dl class="simple">
<dt>X: array_like, shape=[n_samples, n_features]</dt><dd><p>Training data</p>
</dd>
<dt>y: array_like, shape=[n_samples,]</dt><dd><p>Training targets</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.get_keras_model">
<span class="sig-name descname"><span class="pre">get_keras_model</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">include_output</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.get_keras_model" title="Permalink to this definition"></a></dt>
<dd><p>Returns the entire Keras model or the model without the output layer in 
its current state (fitted or compiled)</p>
<dl class="simple">
<dt>include_output: bool</dt><dd><p>if True it will return the entire model, if False it will return 
the model without the output layer</p>
</dd>
</dl>
<p>self.model: tensorflow.python.keras type object</p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.load">
<span class="sig-name descname"><span class="pre">load</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path_to_model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.load" title="Permalink to this definition"></a></dt>
<dd><p>Loads the chemml.models.MLP object along with the underlying 
tensorflow.python.keras object</p>
<dl class="simple">
<dt>path_to_model: str</dt><dd><p>path to the chemml.models.MLP object</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.parse_layer_config">
<span class="sig-name descname"><span class="pre">parse_layer_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">layer_config_file</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.parse_layer_config" title="Permalink to this definition"></a></dt>
<dd><p>Internal method to parse a layer config file</p>
<dl class="simple">
<dt>layer_config_file: str</dt><dd><p>Filepath that contains the layer configuration file - Refer MLP test to see a sample file
Refer MLP test to see a sample file and <a class="reference external" href="https://keras.io/layers/about-keras-layers/">https://keras.io/layers/about-keras-layers/</a>
for all possible types of layers and corresponding layer parameters</p>
</dd>
</dl>
<dl class="simple">
<dt>layers: list</dt><dd><p>List of tuples containing layer type and dictionary of layer parameter arguments</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.parse_opt_config">
<span class="sig-name descname"><span class="pre">parse_opt_config</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">opt_config</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.parse_opt_config" title="Permalink to this definition"></a></dt>
<dd><p>Internal method to parse a optimizer config file</p>
<dl class="simple">
<dt>opt_config: list</dt><dd><p>optimizer configuration (for e.g., [“Adam”,{“learning_rate”:0.01}] 
or [“SGD”,{“lr”:0.01, “momentum”:0.9, “lr_decay”:0.0, nesterov=False)
refer <a class="reference external" href="https://keras.io/optimizers/">https://keras.io/optimizers/</a> for all possible types of 
optimizers and corresponding optimizer parameters</p>
</dd>
</dl>
<dl class="simple">
<dt>opt: keras.optimizers</dt><dd><p>keras optimizer created out of contents of optmizer configuration file</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.predict" title="Permalink to this definition"></a></dt>
<dd><p>Return prediction for test data X</p>
<dl class="simple">
<dt>X: array_like, shape=[n_samples, n_features]</dt><dd><p>Testing data</p>
</dd>
</dl>
<dl class="simple">
<dt>float</dt><dd><p>Predicted value from model</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.save">
<span class="sig-name descname"><span class="pre">save</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filename</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.save" title="Permalink to this definition"></a></dt>
<dd><p>Saves the chemml.models.MLP object along with the underlying 
tensorflow.python.keras object</p>
<dl class="simple">
<dt>path: str</dt><dd><p>the path to the directory where the models should be saved</p>
</dd>
<dt>filename: str</dt><dd><p>the name of the model file without the file type</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.MLP.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.MLP.score" title="Permalink to this definition"></a></dt>
<dd><p>Predict results for test data X and compare with true targets y. Returns root mean square error if regression,
accuracy if classification</p>
<dl class="simple">
<dt>X: array_like, shape=[n_samples, n_features]</dt><dd><p>Test data</p>
</dd>
<dt>y: array_like, shape=[n_samples,]</dt><dd><p>True targets</p>
</dd>
</dl>
<dl class="simple">
<dt>float</dt><dd><p>root mean square error if regression, accuracy if classification</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">chemml.models.</span></span><span class="sig-name descname"><span class="pre">NeuralGraphHidden</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden" title="Permalink to this definition"></a></dt>
<dd><dl>
<dt>Hidden Convolutional layer in a Neural Graph (as in Duvenaud et. al.,</dt><dd><p>2015). This layer takes a graph as an input. The graph is represented as by
three tensors.
- The atoms tensor represents the features of the nodes.
- The bonds tensor represents the features of the edges.
- The edges tensor represents the connectivity (which atoms are connected to</p>
<blockquote>
<div><p>which)</p>
</div></blockquote>
<p>It returns the convolved features tensor, which is very similar to the atoms
tensor. Instead of each node being represented by a num_atom_features-sized
vector, each node now is represented by a convolved feature vector of size
conv_width.
# Example</p>
<blockquote>
<div><p>Define the input:
<a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python</p>
<blockquote>
<div><p>atoms0 = Input(name=’atom_inputs’, shape=(max_atoms, num_atom_features))
bonds = Input(name=’bond_inputs’, shape=(max_atoms, max_degree, num_bond_features))
edges = Input(name=’edge_inputs’, shape=(max_atoms, max_degree), dtype=’int32’)</p>
</div></blockquote>
<p><a href="#id5"><span class="problematic" id="id6">``</span></a>`
The <cite>NeuralGraphHidden</cite> can be initialised in three ways:
1. Using an integer <cite>conv_width</cite> and possible kwags (<cite>Dense</cite> layer is used)</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">atoms1</span> <span class="pre">=</span> <span class="pre">NeuralGraphHidden(conv_width,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False)([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><dl class="simple">
<dt>Using an initialised <cite>Dense</cite> layer</dt><dd><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">atoms1</span> <span class="pre">=</span> <span class="pre">NeuralGraphHidden(Dense(conv_width,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False))([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Using a function that returns an initialised <cite>Dense</cite> layer</dt><dd><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">atoms1</span> <span class="pre">=</span> <span class="pre">NeuralGraphHidden(lambda:</span> <span class="pre">Dense(conv_width,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False))([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</li>
</ol>
<p>Use <cite>NeuralGraphOutput</cite> to convert atom layer to fingerprint</p>
</div></blockquote>
<dl>
<dt># Arguments</dt><dd><dl class="simple">
<dt>inner_layer_arg: Either:</dt><dd><ol class="arabic simple">
<li><dl class="simple">
<dt>an int defining the <cite>conv_width</cite>, with optional kwargs for the</dt><dd><p>inner Dense layer</p>
</dd>
</dl>
</li>
<li><p>An initialised but not build (<cite>Dense</cite>) keras layer (like a wrapper)</p></li>
<li><p>A function that returns an initialised keras layer.</p></li>
</ol>
</dd>
</dl>
<p>kwargs: For initialisation 1. you can pass <cite>Dense</cite> layer kwargs</p>
</dd>
<dt># Input shape</dt><dd><p>List of Atom and edge tensors of shape:
<a href="#id7"><span class="problematic" id="id8">`</span></a>[(samples, max_atoms, atom_features), (samples, max_atoms, max_degrees,</p>
<blockquote>
<div><p>bond_features), (samples, max_atoms, max_degrees)]`</p>
</div></blockquote>
<p>where degrees referes to number of neighbours</p>
</dd>
<dt># Output shape</dt><dd><p>New atom featuers of shape
<cite>(samples, max_atoms, conv_width)</cite></p>
</dd>
<dt># References</dt><dd><ul class="simple">
<li><p>[Convolutional Networks on Graphs for Learning Molecular Fingerprints](<a class="reference external" href="https://arxiv.org/abs/1509.09292">https://arxiv.org/abs/1509.09292</a>)</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden.build" title="Permalink to this definition"></a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden.call" title="Permalink to this definition"></a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>inputs: Input tensor, or dict/list/tuple of input tensors.</dt><dd><p>The first positional <cite>inputs</cite> argument is subject to special rules:
- <cite>inputs</cite> must be explicitly passed. A layer cannot have zero</p>
<blockquote>
<div><p>arguments, and <cite>inputs</cite> cannot be provided via the default value
of a keyword argument.</p>
</div></blockquote>
<ul class="simple">
<li><p>NumPy array or Python scalar values in <cite>inputs</cite> get cast as tensors.</p></li>
<li><p>Keras mask metadata is only collected from <cite>inputs</cite>.</p></li>
<li><p>Layers are built (<cite>build(input_shape)</cite> method)
using shape info from <cite>inputs</cite> only.</p></li>
<li><p><cite>input_spec</cite> compatibility is only checked against <cite>inputs</cite>.</p></li>
<li><p>Mixed precision input casting is only applied to <cite>inputs</cite>.
If a layer has tensor arguments in <cite>*args</cite> or <cite>**kwargs</cite>, their
casting behavior in mixed precision should be handled manually.</p></li>
<li><p>The SavedModel input specification is generated using <cite>inputs</cite> only.</p></li>
<li><p>Integration with various ecosystem packages like TFMOT, TFLite,
TF.js, etc is only supported for <cite>inputs</cite> and not for tensors in
positional and keyword arguments.</p></li>
</ul>
</dd>
<dt><a href="#id9"><span class="problematic" id="id10">*</span></a>args: Additional positional arguments. May contain tensors, although</dt><dd><p>this is not recommended, for the reasons above.</p>
</dd>
<dt><a href="#id11"><span class="problematic" id="id12">**</span></a>kwargs: Additional keyword arguments. May contain tensors, although</dt><dd><p>this is not recommended, for the reasons above.
The following optional keyword arguments are reserved:
- <cite>training</cite>: Boolean scalar tensor of Python boolean indicating</p>
<blockquote>
<div><p>whether the <cite>call</cite> is meant for training or inference.</p>
</div></blockquote>
<ul class="simple">
<li><p><cite>mask</cite>: Boolean input mask. If the layer’s <cite>call()</cite> method takes a
<cite>mask</cite> argument, its default value will be set to the mask generated
for <cite>inputs</cite> by the previous layer (if <cite>input</cite> did come from a layer
that generated a corresponding mask, i.e. if it came from a Keras
layer with masking support).</p></li>
</ul>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden.compute_output_shape">
<span class="sig-name descname"><span class="pre">compute_output_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden.compute_output_shape" title="Permalink to this definition"></a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>If the layer has not been built, this method will call <cite>build</cite> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>input_shape: Shape tuple (tuple of integers)</dt><dd><p>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>An input shape tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphHidden.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphHidden.get_config" title="Permalink to this definition"></a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<p>Note that <cite>get_config()</cite> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">chemml.models.</span></span><span class="sig-name descname"><span class="pre">NeuralGraphOutput</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="o"><span class="pre">*</span></span><span class="n"><span class="pre">args</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput" title="Permalink to this definition"></a></dt>
<dd><p>Output Convolutional layer in a Neural Graph (as in Duvenaud et. al.,
2015). This layer takes a graph as an input. The graph is represented as by
three tensors.</p>
<ul class="simple">
<li><p>The atoms tensor represents the features of the nodes.</p></li>
<li><p>The bonds tensor represents the features of the edges.</p></li>
<li><dl class="simple">
<dt>The edges tensor represents the connectivity (which atoms are connected to</dt><dd><p>which)</p>
</dd>
</dl>
</li>
</ul>
<p>It returns the fingerprint vector for each sample for the given layer.</p>
<p>According to the original paper, the fingerprint outputs of each hidden layer
need to be summed in the end to come up with the final fingerprint.</p>
<dl>
<dt># Example</dt><dd><p>Define the input:
<a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a>python</p>
<blockquote>
<div><p>atoms0 = Input(name=’atom_inputs’, shape=(max_atoms, num_atom_features))
bonds = Input(name=’bond_inputs’, shape=(max_atoms, max_degree, num_bond_features))
edges = Input(name=’edge_inputs’, shape=(max_atoms, max_degree), dtype=’int32’)</p>
</div></blockquote>
<p><a href="#id17"><span class="problematic" id="id18">``</span></a><a href="#id19"><span class="problematic" id="id20">`</span></a></p>
<p>The <cite>NeuralGraphOutput</cite> can be initialised in three ways:
1. Using an integer <cite>fp_length</cite> and possible kwags (<cite>Dense</cite> layer is used)</p>
<blockquote>
<div><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">fp_out</span> <span class="pre">=</span> <span class="pre">NeuralGraphOutput(fp_length,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False)([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><dl class="simple">
<dt>Using an initialised <cite>Dense</cite> layer</dt><dd><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">fp_out</span> <span class="pre">=</span> <span class="pre">NeuralGraphOutput(Dense(fp_length,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False))([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>Using a function that returns an initialised <cite>Dense</cite> layer</dt><dd><p><code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">fp_out</span> <span class="pre">=</span> <span class="pre">NeuralGraphOutput(lambda:</span> <span class="pre">Dense(fp_length,</span> <span class="pre">activation='relu',</span> <span class="pre">bias=False))([atoms0,</span> <span class="pre">bonds,</span> <span class="pre">edges])</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</li>
</ol>
<p>Predict for regression:
<code class="docutils literal notranslate"><span class="pre">`python</span>
<span class="pre">main_prediction</span> <span class="pre">=</span> <span class="pre">Dense(1,</span> <span class="pre">activation='linear',</span> <span class="pre">name='main_prediction')(fp_out)</span>
<span class="pre">`</span></code></p>
</dd>
<dt># Arguments</dt><dd><dl class="simple">
<dt>inner_layer_arg: Either:</dt><dd><ol class="arabic simple">
<li><dl class="simple">
<dt>an int defining the <cite>fp_length</cite>, with optional kwargs for the</dt><dd><p>inner Dense layer</p>
</dd>
</dl>
</li>
<li><p>An initialised but not build (<cite>Dense</cite>) keras layer (like a wrapper)</p></li>
<li><p>A function that returns an initialised keras layer.</p></li>
</ol>
</dd>
</dl>
<p>kwargs: For initialisation 1. you can pass <cite>Dense</cite> layer kwargs</p>
</dd>
<dt># Input shape</dt><dd><p>List of Atom and edge tensors of shape:
<a href="#id21"><span class="problematic" id="id22">`</span></a>[(samples, max_atoms, atom_features), (samples, max_atoms, max_degrees,</p>
<blockquote>
<div><p>bond_features), (samples, max_atoms, max_degrees)]`</p>
</div></blockquote>
<p>where degrees referes to number of neighbours</p>
</dd>
<dt># Output shape</dt><dd><p>Fingerprints matrix
<cite>(samples, fp_length)</cite></p>
</dd>
<dt># References</dt><dd><ul class="simple">
<li><p>[Convolutional Networks on Graphs for Learning Molecular Fingerprints](<a class="reference external" href="https://arxiv.org/abs/1509.09292">https://arxiv.org/abs/1509.09292</a>)</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput.build">
<span class="sig-name descname"><span class="pre">build</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput.build" title="Permalink to this definition"></a></dt>
<dd><p>Creates the variables of the layer (optional, for subclass implementers).</p>
<p>This is a method that implementers of subclasses of <cite>Layer</cite> or <cite>Model</cite>
can override if they need a state-creation step in-between
layer instantiation and layer call.</p>
<p>This is typically used to create the weights of <cite>Layer</cite> subclasses.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>input_shape: Instance of <cite>TensorShape</cite>, or list of instances of</dt><dd><p><cite>TensorShape</cite> if the layer expects a list of inputs
(one instance per input).</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput.call">
<span class="sig-name descname"><span class="pre">call</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mask</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput.call" title="Permalink to this definition"></a></dt>
<dd><p>This is where the layer’s logic lives.</p>
<p>Note here that <cite>call()</cite> method in <cite>tf.keras</cite> is little bit different
from <cite>keras</cite> API. In <cite>keras</cite> API, you can pass support masking for
layers as additional arguments. Whereas <cite>tf.keras</cite> has <cite>compute_mask()</cite>
method to support masking.</p>
<dl>
<dt>Args:</dt><dd><dl>
<dt>inputs: Input tensor, or dict/list/tuple of input tensors.</dt><dd><p>The first positional <cite>inputs</cite> argument is subject to special rules:
- <cite>inputs</cite> must be explicitly passed. A layer cannot have zero</p>
<blockquote>
<div><p>arguments, and <cite>inputs</cite> cannot be provided via the default value
of a keyword argument.</p>
</div></blockquote>
<ul class="simple">
<li><p>NumPy array or Python scalar values in <cite>inputs</cite> get cast as tensors.</p></li>
<li><p>Keras mask metadata is only collected from <cite>inputs</cite>.</p></li>
<li><p>Layers are built (<cite>build(input_shape)</cite> method)
using shape info from <cite>inputs</cite> only.</p></li>
<li><p><cite>input_spec</cite> compatibility is only checked against <cite>inputs</cite>.</p></li>
<li><p>Mixed precision input casting is only applied to <cite>inputs</cite>.
If a layer has tensor arguments in <cite>*args</cite> or <cite>**kwargs</cite>, their
casting behavior in mixed precision should be handled manually.</p></li>
<li><p>The SavedModel input specification is generated using <cite>inputs</cite> only.</p></li>
<li><p>Integration with various ecosystem packages like TFMOT, TFLite,
TF.js, etc is only supported for <cite>inputs</cite> and not for tensors in
positional and keyword arguments.</p></li>
</ul>
</dd>
<dt><a href="#id23"><span class="problematic" id="id24">*</span></a>args: Additional positional arguments. May contain tensors, although</dt><dd><p>this is not recommended, for the reasons above.</p>
</dd>
<dt><a href="#id25"><span class="problematic" id="id26">**</span></a>kwargs: Additional keyword arguments. May contain tensors, although</dt><dd><p>this is not recommended, for the reasons above.
The following optional keyword arguments are reserved:
- <cite>training</cite>: Boolean scalar tensor of Python boolean indicating</p>
<blockquote>
<div><p>whether the <cite>call</cite> is meant for training or inference.</p>
</div></blockquote>
<ul class="simple">
<li><p><cite>mask</cite>: Boolean input mask. If the layer’s <cite>call()</cite> method takes a
<cite>mask</cite> argument, its default value will be set to the mask generated
for <cite>inputs</cite> by the previous layer (if <cite>input</cite> did come from a layer
that generated a corresponding mask, i.e. if it came from a Keras
layer with masking support).</p></li>
</ul>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>A tensor or list/tuple of tensors.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput.compute_output_shape">
<span class="sig-name descname"><span class="pre">compute_output_shape</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">inputs_shape</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput.compute_output_shape" title="Permalink to this definition"></a></dt>
<dd><p>Computes the output shape of the layer.</p>
<p>If the layer has not been built, this method will call <cite>build</cite> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>
<dl class="simple">
<dt>Args:</dt><dd><dl class="simple">
<dt>input_shape: Shape tuple (tuple of integers)</dt><dd><p>or list of shape tuples (one per output tensor of the layer).
Shape tuples can include None for free dimensions,
instead of an integer.</p>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>An input shape tuple.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.NeuralGraphOutput.get_config">
<span class="sig-name descname"><span class="pre">get_config</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.NeuralGraphOutput.get_config" title="Permalink to this definition"></a></dt>
<dd><p>Returns the config of the layer.</p>
<p>A layer config is a Python dictionary (serializable)
containing the configuration of a layer.
The same layer can be reinstantiated later
(without its trained weights) from this configuration.</p>
<p>The config of a layer does not include connectivity
information, nor the layer class name. These are handled
by <cite>Network</cite> (one layer of abstraction above).</p>
<p>Note that <cite>get_config()</cite> does not guarantee to return a fresh copy of dict
every time it is called. The callers should make a copy of the returned dict
if they want to modify it.</p>
<dl class="simple">
<dt>Returns:</dt><dd><p>Python dictionary.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

<dl class="py class">
<dt class="sig sig-object py" id="chemml.models.TransferLearning">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">chemml.models.</span></span><span class="sig-name descname"><span class="pre">TransferLearning</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">base_model</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_features</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.TransferLearning" title="Permalink to this definition"></a></dt>
<dd><p>Class used to facilitate transfer learning from a parent (or head) model to 
a child model.Freezes the layers (weights and biases) of the pre-trained base 
model and removes the input and output layers and adds it to the child model.</p>
<dl class="simple">
<dt>base_model: chemml.models.keras.MLP object or tensorflow.python.keras object</dt><dd><p>pre-trained base model</p>
</dd>
<dt>n_features: int or None, default: None</dt><dd><p>no. of input features provided for training the base model. If base_model
is a tensorflow.python.keras object then n_features must be int, else it 
can be None.</p>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="chemml.models.TransferLearning.transfer">
<span class="sig-name descname"><span class="pre">transfer</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">X</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">y</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">child_model</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#chemml.models.TransferLearning.transfer" title="Permalink to this definition"></a></dt>
<dd><p>Adds the base model’s frozen layers (without its input and output layers) 
to the child model and fits the new model to the training data.</p>
<dl class="simple">
<dt>X, y: numpy.ndarray</dt><dd><p>X is an array of features and y is an array of the target values</p>
</dd>
<dt>child_model: chemml.models.keras.mlp.MLP object</dt><dd><p>chemml model created with all the parameters and options required for 
the final transfer learned model</p>
</dd>
</dl>
<dl class="simple">
<dt>child_model: chemml.models.keras.mlp.MLP object</dt><dd><p>The trained transfer-learned child model.</p>
</dd>
</dl>
</dd></dl>

</dd></dl>

</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="chemml.preprocessing.html" class="btn btn-neutral float-left" title="Preprocessing module" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="chemml.optimization.html" class="btn btn-neutral float-right" title="Optimization module" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2014-2021 Johannes Hachmann, Mojtaba Haghighatlari.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>